# -*- coding: utf-8 -*-
"""Workshop_DUOC_25-04-25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rh4DTnqkAzW4ONrIUZd-f_vWK9YLE3hW

# **Desarrollando un ChatBot con OpenAI y Langchain**
En este workshop trabajaremos en desarrollar distintos chatbots con OpenAI y LangChain. Revisaremos los siguientes temas:


*   Modelos de Lenguaje (LLMs)
*   Prompt Engineering
*   Retrieval-Augmented Generation (RAG)
*   Embeddings - Búsqueda Semántica
*   Agentes con tools y memoria.

Las instrucciones y descripciones detalladas las pueden encontrar en el siguiente [blog](https://cristobalchavez21.hashnode.dev/desarrollando-un-chatbot-con-openai-y-langchain)

### Setup
"""

# Commented out IPython magic to ensure Python compatibility.

# %pip install --upgrade --quiet langchain langchain-openai langchain_google_community langchain_community langchainhub pypdf faiss-cpu tiktoken google-search-results wikipedia

import os
os.environ["OPENAI_API_KEY"] = "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"

"""## Large Language Models (LLMs)
Los LLMs como GPT son redes neuronales entrenadas para entender y generar texto de forma similar a cómo lo haría un humano. Estas redes neuronales son muy complejas y requieren de enormes cantidades de información para entrenarse.

El parámetro de temperatura es un número entre 0 y 1 que controla la aleatoriedad de las respuestas generadas. Una temperatura alta produce respuestas más diversas pero a veces menos coherentes, mientras que una temperatura baja produce respuestas más predecibles pero potencialmente repetitivas. Puede ser interpretado como la "creatividad" del modelo. Es importante tener en cuenta que, incluso con una temperatura 0, igual habrá cierto grado de aleatoriedad y el modelo puede generar respuestas variadas.

Probemos cómo afecta las respuestas
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0, model="gpt-4o-mini", seed=2101)

model.invoke("cuentame un chiste sobre perros").content

model.invoke("cuentame un chiste sobre perros").content

model.invoke("cuentame un chiste sobre perros").content

"""### LangChain
De aquí en adelante usaremos cadenas de Langchain. Se componen de varias partes, pero las más importantes son el prompt y el modelo.

* prompt: Aquí va toda la información que se le entregará al LLM, incluyendo las instrucciones, el contexto y el mensaje del usuario. Los prompt generalmente se crean en base a templates, lo que permite tener la misma instrucción base para varios prompts distintos dependiendo del input.

* modelo: el modelo de lenguaje que generará la respuesta (gpt, bison, claude, etc). Una ventaja de Langchain es que permite crear cadenas que funcionan con distintos modelos.
"""

prompt_template = """Cuéntame una historia sobre {tema}"""
prompt = ChatPromptTemplate.from_template(prompt_template)
model = ChatOpenAI(temperature=0, model="gpt-4o-mini", seed=2101)
chain = prompt | model

chain.invoke({"tema": "perros"}).content

chain.invoke({"tema": "robots"}).content

"""## Prompt Engineering
Para tareas más complejas, como la resolución de problemas matemáticos, las instrucciones que le damos en el prompt pueden mejorar (o empeorar) mucho la calidad de la respuesta. Encontrar el prompt que genere los mejores resultados es una tarea difícil y se le llama prompt engineering.

Probemos distintas técnicas de prompting
"""

# prompt_template = """Responde la pregunta. Responde en texto plano sin formatos como markdown.
# Problema: {ecuacion}
# Respuesta:"""

# prompt_template = """Eres un excelente matemático, capaz de resolver \
# cualquier problema. Responde en texto plano sin formatos como markdown.
# Resuelve el siguiente problema: {ecuacion}
# Solución:"""

prompt_template = """Dado el siguiente problema o ecuación matemática, \
plantea cómo solucionarlo paso a paso, luego responde la pregunta según tu razonamiento. Piensa detenidamente cuál es la mejor forma de resolver el problema. \
Responde en texto plano sin formatos como markdown. \
Responde con tu razonamiento paso a paso, y luego con tu respuesta.
Problema: {ecuacion}
Respuesta:"""

model = ChatOpenAI(temperature=0, model="gpt-4o-mini", seed=1)
prompt = ChatPromptTemplate.from_template(prompt_template)
chain = prompt | model

"""**Y si le preguntamos sobre algo que no conoce?**

Los modelos de lenguaje calculan las palabras que son más probables a seguir un texto. Por lo tanto generará la respuesta "más probable" incluso si no "conoce" la respuesta real. A esto se le llama alucinación.
"""

#x = (3 + √29) / 2 y x = (3 - √29) / 2.
print(chain.invoke({"ecuacion": "x^2-3x+2=7"}).content)

# 13
print(chain.invoke(
    {"ecuacion": "Cuánto suman todos los dígitos primos del número 9233849230"}).content)

#186
print(chain.invoke(
    {"ecuacion": "cuantas fechas impares tiene el año"}).content)

"""## Retrieval-Augmented Generation (RAG)
Retrieval-Augmented Generation es una técnica que combina la generación de texto con la búsqueda semántica. Funciona mediante la creación de embeddings (representaciones vectoriales) de los documentos relevantes y luego utiliza estos embeddings para recuperar información relevante durante el proceso de generación de texto.

"""

prompt_template = """Eres un experto en cine. Conoces todas las películas del \
mundo. Responde la siguiente pregunta realcionada al cine.
Pregunta: {pregunta}"""
prompt = ChatPromptTemplate.from_template(prompt_template)
chain = prompt | model
print(chain.invoke({"pregunta": "Cuántos oscars ha ganado Emma Stone"}).content)

prompt_template = """Eres un asistente del DUOC UC, que puede responder \
preguntas sobre el reglamento de la institución. Responde la siguiente pregunta.
Pregunta: {pregunta}"""
prompt = ChatPromptTemplate.from_template(prompt_template)
chain = prompt | model
print(chain.invoke({"pregunta": "Cuál es la asistencia mínima para aprobar una \
asignatura?"}).content)

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/RES-VRA-03-2024-NUEVO-REGLAMENTO-ACADÉMICO63-1.pdf")
pages = loader.load_and_split()

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# Funcion para crear embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
)

# Creamos una base de datos vectorial a partir de las páginas del documento
faiss_index = FAISS.from_documents(pages, embeddings)

from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
template = """Eres un asistente del DUOC UC, que puede responder preguntas \
sobre el reglamento de la institución. Responde a la pregunta basándote solo \
en el contexto siguiente:
{context}

Pregunta: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
retriever = faiss_index.as_retriever()
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

template = """Eres un asistente del DUOC UC, que puede responder preguntas \
sobre el reglamento de la institución. Responde a la pregunta basándote solo \
en el contexto siguiente:
Pag 1 doc virtual:
setenta por ciento (70 %) de asistencia de las actividades teóricas y un setenta por ciento (70 %) de
asistencia de las actividades prácticas. Se excluyen de lo dispuesto en este inciso, las prácticas
laborales, profesionales, clínicas e internados.

Pag 1 doc presencial:

Pregunta: {question}
"""

import langchain
langchain.debug = False

chain.invoke("Cuál es la política de asistencia")

chain.invoke("Se puede beber alcohol?")

chain.invoke("Cuántos autos puedo estacionar en el campus")

chain.invoke("me llamo cristobal")

chain.invoke("Cómo me llamo?")

"""## Agente + tools + memoria
Los agentes pueden ejecutar funciones y ocupar herramientas externas, dándole más capacidades al asistente.

En el siguiente ejemplo crearemos una agente que puede realizar búsquedas en google, en wikipedia y usar el retreiver que definimos anteriormente.

Para esta parte necesitaremos una API KEY de serper, que pueden conseguir creando una cuenta gratuita [aqui](https://serper.dev/).
"""

os.environ["SERPER_API_KEY"] = "4370b7fedb92f4299cee6b9b21fe0953c2475d37"

from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentType, initialize_agent
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_core.tools import Tool
from langchain_openai import OpenAI
from langchain.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from langchain.agents import tool
from datetime import datetime
import pytz
from langchain.tools.base import StructuredTool
from langchain_core.pydantic_v1 import BaseModel, Field

@tool
def dias_hasta_18_septiembre() -> int:
    """Entrega la cantidad de días faltantes hasta el próximo 18 de septiembre."""
    hoy = datetime.now()
    proximo_18_septiembre = datetime(hoy.year, 9, 18)

    # Si ya pasó el 18 de septiembre de este año, sumamos un año
    if hoy > proximo_18_septiembre:
        proximo_18_septiembre = proximo_18_septiembre.replace(year=hoy.year + 1)

    dias_faltantes = (proximo_18_septiembre - hoy).days
    return dias_faltantes

@tool
def current_datetime() -> int:
    """Entrega la fecha y hora actual en Chile continental. Debes usar esta función con \
cada pregunta que requiera un contexto temporal, como preguntas preguntando por 'eventos recientes' o 'la ultima ocurrencia de algo'."""
    zona_horaria_chile = pytz.timezone('America/Santiago')

    # Obtener la hora actual en esa zona horaria
    hora_chile = datetime.now(zona_horaria_chile)

    # Formatear la hora en el formato deseado
    return hora_chile.strftime('%Y-%m-%d %H:%M:%S')

retriever_tool = create_retriever_tool(
    retriever,
    "busqueda_reglamento_DUOC",
    "Busca información sobre el reglamento académico del DUOC UC. Debes usar \
esta herramienta para cualquier pregunta relacionada al reglamento del \
instituto DUOC UC.",
)

wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
wikipedia_tool.description = "Realiza una búsqueda en wikipedia. Sirve \
cuando necesites encontrar información sobre una persona o tema en específico. La pregunta debe estar en inglés."

search = GoogleSerperAPIWrapper()

class Search(BaseModel):
  query: str

search_tool = StructuredTool(
    name='search_tool',
    func=search.run,
    description="Realiza una búsqueda en google. Sirve cuando necesitas \
buscar algo en google. Úsala cuando necesites información de actualidad u otra informacion que no conozcas.",
    args_schema=Search,
)

tools = [wikipedia_tool, retriever_tool, dias_hasta_18_septiembre, current_datetime, search_tool]

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor, create_openai_tools_agent
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. You may not need to use tools \
for every query - the user may just want to chat. Don't assume the current year, you should use tools to get the current date. \
Think step by step how to obtain the information you need. You may need to call more than one tool, or the same tool multiple times. \
Only respond to the user when you are sure of your answer.",
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)
model = ChatOpenAI(temperature=0, model="gpt-4o-mini", seed=211)
agent = create_openai_tools_agent(model, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

from langchain.memory import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

memory = ChatMessageHistory()

conversational_agent_executor = RunnableWithMessageHistory(
    agent_executor,
    lambda session_id: memory,
    input_messages_key="input",
    output_messages_key="output",
    history_messages_key="chat_history",
)

def send_question(pregunta):
  print(conversational_agent_executor.invoke(
    {
        "input": pregunta,
    },
    {"configurable": {"session_id": "sesion1"}},
)["output"])

send_question("cuántos oscars tiene emma stone")

send_question("Cuántos años tiene")

send_question("qué hora es en chile, en santiago")

send_question("Se puede beber alcohol en el DUOC UC?")

send_question("cuanto falta para el 18")

